_target_: glm_experiments.models.bert_lit_module.BERTLitModule

net:
  _target_: glm_experiments.models.components.bert.BERT
  embedder:
    _target_: glm_experiments.models.components.transformer.Embedding
    vocab_size: 7
    d_model: 768 # Standard BERT-base size
  encoder:
    _target_: glm_experiments.models.components.transformer.Transformer
    hidden_size: ${..embedder.d_model} # 768
    n_layers: 12 # CS336 default
    num_heads: 12 # 12 heads â†’ d_head = 64
    # d_ff: auto-computed as floor(768 * 8/3 / 64) * 64 = 2048
    rope_theta: 10000.0
    is_causal: false
  layer_norm:
    _target_: torch.nn.RMSNorm
    normalized_shape: ${..embedder.d_model}
  decoder:
    _target_: glm_experiments.models.components.transformer.Linear
    d_in: ${..embedder.d_model}
    d_out: ${..embedder.vocab_size}

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.001 # CS336 default
  weight_decay: 0.1 # CS336 default
  betas: [0.9, 0.98] # CS336 default (beta1, beta2)
  eps: 1.0e-9 # CS336 default

scheduler:
  _target_: transformers.get_constant_schedule_with_warmup
  _partial_: true
  num_warmup_steps: 1000 # More warmup for larger model
