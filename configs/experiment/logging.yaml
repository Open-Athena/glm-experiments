# @package _global_

logger:
  wandb:
    name: debug-lr-gradnorm-logging

trainer:
  max_steps: 100
  log_every_n_steps: 5
  val_check_interval: 5
  limit_val_batches: 2
  check_val_every_n_epoch: null

model:
  net:
    embedder:
      embedding_dim: 32
    encoder:
      n_layers: 2
  scheduler:
    _target_: transformers.get_cosine_schedule_with_warmup
    _partial_: true
    num_warmup_steps: 10
    num_training_steps: ${trainer.max_steps}

data:
  batch_size: 8
  per_device_batch_size: 8

compile: false
