# @package _global_

# to execute this experiment run:
# python glm_experiments/train.py experiment=transformer_base

defaults:
  - override /data: gpn_animal_promoter
  - override /model: dlm_transformer_base
  - override /trainer: gpn_animal_promoter

logger:
  wandb:
    name: experiment-dlm-transformer-base
    tags: ["experiment", "dlm", "transformer", "base"]

data:
  _target_: glm_experiments.data.lm_datamodule.DLMDataModule
  per_device_batch_size: 256

model:
  scheduler:
    _target_: transformers.get_cosine_with_min_lr_schedule_with_warmup
    _partial_: true
    num_warmup_steps: 2000
    num_training_steps: ${trainer.max_steps}
    min_lr_rate: 0.1 # Decay to 10% of max lr

trainer:
  max_steps: 20000
  log_every_n_steps: 1000
  val_check_interval: 1000
