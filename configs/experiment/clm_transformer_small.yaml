# @package _global_

# Short training run with small Transformer encoder for quick testing

defaults:
  - override /model: clm_transformer_small

logger:
  wandb:
    name: debug-clm-transformer-small
    tags: ["debug", "clm", "transformer", "small"]

trainer:
  max_steps: 300
  log_every_n_steps: 100
  val_check_interval: 100
  limit_val_batches: 10
  check_val_every_n_epoch: null

model:
  net:
    embedder:
      d_model: 32
    encoder:
      n_layers: 2
  scheduler:
    _target_: transformers.get_cosine_schedule_with_warmup
    _partial_: true
    num_warmup_steps: 10
    num_training_steps: ${trainer.max_steps}

data:
  _target_: glm_experiments.data.lm_datamodule.CLMDataModule
  batch_size: 8
  per_device_batch_size: 8
  soft_masked_weight: 0.5

compile: false
